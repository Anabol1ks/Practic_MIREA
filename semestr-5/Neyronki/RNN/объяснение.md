# Объяснение рекуррентной нейронной сети (РНС)

Рекуррентные нейронные сети (РНС) — это особый тип нейронных сетей, которые **имеют обратные связи**, что позволяет им учитывать информацию о предыдущих шагах, или "память", при обработке последовательностей данных. В отличие от традиционных нейронных сетей, которые предполагают независимость каждого входного элемента, РНС эффективно справляются с задачами, где порядок данных важен, например, с временными рядами, текстом или сигналами.

## Основные этапы работы рекуррентной сети (RNN)

1. **Прямое распространение**: 
   - Каждый входной вектор обрабатывается, и выход текущего слоя зависит от входа и состояния сети на предыдущем шаге. Это состояние называется **скрытым состоянием**.
   
2. **Скрытое состояние**:
   - Скрытое состояние хранит информацию о предыдущих входах и передаёт её на следующий шаг, позволяя сети учитывать "память" о прошлых данных.

3. **Обратное распространение во времени (BPTT)**:
   - Обучение выполняется на каждом шаге, используя ошибки текущего и предыдущих шагов для корректировки весов. Этот процесс называется **обратным распространением во времени** (Backpropagation Through Time).

## Задача
В данном примере создаётся простая рекуррентная сеть, которая обрабатывает логическую операцию И. Входные данные будут последовательностью значений, и сеть будет обновлять скрытое состояние на каждом шаге.

---

# Как построить рекуррентную сеть на основе предыдущих кодов

Рассмотрим пошаговый процесс построения сети.

1. **Инициализация данных и параметров**:
   - Мы инициализируем веса для `weightsInput`, `weightsHidden` и `weightsOutput`, а также скрытое состояние `hiddenState` со случайными значениями.
   
2. **Создание рекуррентного слоя со скрытым состоянием**:
   - Функция `forward` обновляет скрытое состояние с учётом текущих входных данных и предыдущего состояния, используя заданную функцию активации.

3. **Прямое распространение с учетом скрытого состояния**:
   - На основе обновлённого скрытого состояния и весов выходного слоя рассчитывается предсказанный результат.
   
4. **Обновление весов через обратное распространение во времени**:
   - Веса слоёв `weightsInput`, `weightsHidden`, и `weightsOutput` корректируются в зависимости от ошибки, используя метод обратного распространения во времени.

# Пояснение к коду

### Инициализация данных и весов
- **Входные данные** — это комбинации логической операции И.
- **Веса** `weightsInput`, `weightsHidden`, `weightsOutput` и **скрытое состояние** `hiddenState` инициализируются случайными значениями.

### Прямое распространение
- Функция `forward` обновляет скрытое состояние с учётом предыдущих значений и текущего входа.
- **Выход** рассчитывается как сумма произведений скрытого состояния и весов выходного слоя.

### Обратное распространение во времени
- Функция `backward` использует ошибку на выходе для корректировки весов с учётом функции активации и её производной.
- **Веса** обновляются для слоёв `weightsOutput`, `weightsHidden`, `weightsInput` на основе текущего состояния.

### Цикл обучения
- В каждом шаге сеть обучается на основе входных данных и целевых значений.
- Обновлённые веса и ошибки отображаются на каждом шаге.
