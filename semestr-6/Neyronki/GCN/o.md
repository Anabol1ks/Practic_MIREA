### 1. Загрузка датасета CORA

```python
from torch_geometric.datasets import Planetoid

# Загружаем датасет CORA, который хранится в папке '/tmp/Cora'
dataset = Planetoid(root='/tmp/Cora', name='Cora')
data = dataset[0]  # В датасете CORA весь граф представлен одним объектом
```

**Пояснение:**

- **Planetoid:** Это класс из библиотеки PyTorch Geometric, предназначенный для загрузки датасетов, таких как CORA, Citeseer и PubMed.  
- **root:** Указывает, куда будет сохранён датасет (если он ещё не скачан, он будет загружен).  
- **data:** Объект, содержащий всю информацию о графе. В нём содержится:
  - `data.x` – матрица признаков узлов (каждый узел имеет набор входных признаков).
  - `data.edge_index` – представление графа в виде списка ребер (двумерный тензор, где каждая колонка описывает ребро от одного узла к другому).
  - Маски для тренировочной, валидационной и тестовой выборок (`data.train_mask`, `data.val_mask`, `data.test_mask`).
  - `data.y` – метки классов для каждого узла.

---

### 2. Определение модели GCN

```python
import torch
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class GCN(torch.nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GCN, self).__init__()
        # Первый графовый сверточный слой: преобразует входные признаки в скрытое представление
        self.conv1 = GCNConv(input_dim, hidden_dim)
        # Второй графовый сверточный слой: преобразует скрытое представление в классовые вероятности
        self.conv2 = GCNConv(hidden_dim, output_dim)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        # Применяем первый слой и функцию активации ReLU
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        # Применяем dropout для регуляризации, чтобы избежать переобучения
        x = F.dropout(x, training=self.training)
        # Второй слой преобразует данные в логиты классов
        x = self.conv2(x, edge_index)
        # Применяем log_softmax для получения логарифмов вероятностей по классам
        return F.log_softmax(x, dim=1)
```

**Пояснение:**

- **GCNConv:** Это слой графовой сверточной сети. Он принимает на вход матрицу признаков и структуру графа (edge_index) и вычисляет обновлённые представления для узлов с учётом их соседей.
  
- **Первый слой (`conv1`):**
  - Преобразует исходные признаки узлов (размерность `input_dim`) в скрытое представление размерности `hidden_dim`.
  - После сверточного преобразования применяется функция ReLU, которая добавляет нелинейность.

- **Dropout:** Используется для регуляризации. В режиме обучения случайным образом обнуляются некоторые значения, что помогает предотвратить переобучение.

- **Второй слой (`conv2`):**
  - Преобразует скрытое представление в выходную размерность, равную количеству классов (`output_dim`).
  - После этого применяется функция `log_softmax`, которая выдаёт логарифмы вероятностей для каждого класса по каждому узлу.

---

### 3. Инициализация модели и настройка оптимизатора

```python
# Инициализируем модель: 
# dataset.num_node_features – число входных признаков для каждого узла,
# dataset.num_classes – число классов (размер выходного слоя)
model = GCN(dataset.num_node_features, hidden_dim=16, output_dim=dataset.num_classes)

# Оптимизатор Adam с параметрами lr=0.01 и weight_decay=5e-4 для регуляризации L2
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)
```

**Пояснение:**

- **hidden_dim=16:** Задаёт размерность скрытого пространства. Это число можно варьировать для поиска наилучшей конфигурации.
- **weight_decay:** Это параметр регуляризации, который помогает уменьшить переобучение, добавляя штраф за большие веса модели.

---

### 4. Функция обучения (train)

```python
def train():
    model.train()               # Переводим модель в режим обучения
    optimizer.zero_grad()       # Обнуляем градиенты
    out = model(data)           # Прямой проход по всей сети
    # Вычисляем функцию потерь для узлов, принадлежащих тренировочной выборке
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()             # Обратное распространение ошибки
    optimizer.step()            # Обновление весов модели
    return loss.item()
```

**Пояснение:**

- **model.train():** Переводит модель в режим обучения (например, включается dropout).
- **optimizer.zero_grad():** Обнуляет предыдущие градиенты, чтобы они не накапливались.
- **loss:** Вычисляется с помощью `nll_loss` (negative log likelihood loss), которая совместима с выходом `log_softmax`. Потеря считается только для узлов, выбранных по маске `data.train_mask`.
- **loss.backward() и optimizer.step():** Обновляют веса модели на основе вычисленных градиентов.

---

### 5. Функция тестирования (test)

```python
def test():
    model.eval()                # Переводим модель в режим оценки (выключается dropout)
    logits = model(data)        # Получаем логиты (логарифмы вероятностей) для всех узлов
    accs = []
    for mask in [data.train_mask, data.val_mask, data.test_mask]:
        pred = logits[mask].max(1)[1]  # Выбираем класс с максимальным логитом для каждого узла
        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()  # Считаем точность для текущей выборки
        accs.append(acc)
    return accs
```

**Пояснение:**

- **model.eval():** Переводит модель в режим оценки (выключается dropout, batchnorm и т.д.).
- **logits:** Результат прямого прохода по графу, содержащий логарифмы вероятностей для каждого узла.
- **accs:** Для каждой маски (тренировочная, валидационная, тестовая) вычисляется точность:
  - `max(1)[1]` – выбор индекса с наибольшей вероятностью.
  - Сравнение с истинными метками `data.y[mask]` и вычисление доли правильных предсказаний.

---

### 6. Основной цикл обучения

```python
# Обучаем модель в течение 200 эпох
for epoch in range(1, 201):
    loss = train()
    train_acc, val_acc, test_acc = test()
    if epoch % 10 == 0:
        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '
              f'Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}')
```

**Пояснение:**

- **Цикл:** Модель обучается 200 эпох. На каждой эпохе:
  - Вызывается функция `train()`, которая обновляет веса модели.
  - Функция `test()` вычисляет точность на всех трёх выборках.
- **Вывод каждые 10 эпох:** Позволяет следить за изменениями потерь и точности в процессе обучения.

---

### Итог

- **Что делает модель?**  
  Модель GCN учитывает как признаки узлов, так и структуру графа (ребра между узлами). За счёт применения сверточных операций по графу она способна "обобщать" информацию от соседних узлов, что особенно важно для задач, где локальная структура графа имеет большое значение (например, цитирование научных статей).

- **Почему используется log_softmax?**  
  Применяется функция потерь `nll_loss`, которая ожидает на вход логарифмы вероятностей (вычисленные через `log_softmax`). Это помогает стабилизировать обучение.

- **Регуляризация:**  
  Dropout и L2-регуляризация (через weight_decay в оптимизаторе) помогают предотвратить переобучение модели на тренировочных данных.


---------------
Метод графовой нейронной сети (Graph Neural Network, GNN), в частности архитектура Graph Convolutional Network (GCN), предназначен для обработки данных, представленных в виде графов. Вот в чём суть данного метода:

1. Работа с графовыми структурами:  
   В отличие от обычных нейронных сетей, которые работают с табличными или изображениями, GNN оперируют узлами (вершинами) и связями (рёбрами) графа. Например, в датасете Cora узлы — это научные статьи, а рёбра — ссылки между ними (цитирования).

2. Агрегация информации из соседей:  
   Основная идея GCN заключается в том, что для каждого узла сеть не использует только его собственные признаки, но и агрегирует информацию от соседних узлов. Таким образом, каждый узел «смотрит» на свою локальную структуру графа, что позволяет учитывать контекст и взаимосвязи между данными.

3. Слой графовой свёртки:  
   В архитектуре GCN каждый слой выполняет операцию, подобную свёртке в обычных CNN, но для графовых данных. Графовый свёрточный слой обновляет представление узла, комбинируя его признаки с признаками соседей через специальную нормализованную сумму (агрегацию). Это позволяет «распространять» информацию по графу.

4. Два слоя для классификации:  
   В приведённом примере используется два слоя:
   - Первый слой преобразует исходные признаки узлов в скрытое представление, после чего применяется функция активации ReLU и dropout для повышения устойчивости модели.
   - Второй слой преобразует скрытое представление в логарифмы вероятностей для каждого класса (с помощью log_softmax), что позволяет использовать функцию потерь типа отрицательной логарифмической правдоподобности (nll_loss).

5. Обучение на частично размеченных данных:  
   Обычно в графовых задачах размечены не все узлы. Поэтому модель обучается только на тех узлах, которые имеют метки (train_mask). После обучения точность модели оценивается на валидационной и тестовой выборках, что показывает способность модели обобщать информацию по всему графу.

6. Применимость:  
   Такой подход эффективен для задач, где важны взаимосвязи между объектами, например, классификация научных работ, предсказание социальных связей или рекомендации. Благодаря агрегации информации от соседей, GNN способны учитывать контекст, недоступный традиционным моделям.

Итак, суть метода GNN (и конкретно GCN) состоит в том, чтобы обрабатывать данные в виде графов, учитывая не только индивидуальные признаки объектов, но и их взаимосвязи, что позволяет достичь лучшей производительности в задачах, где структура данных имеет значение.