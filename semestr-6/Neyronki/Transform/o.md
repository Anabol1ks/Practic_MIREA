## I. Объяснение работы Transformer

### 1. Общая концепция  
Transformer — это архитектура нейронных сетей, разработанная для обработки последовательностей (например, текста). В отличие от рекуррентных моделей (RNN, LSTM), Transformer не обрабатывает данные последовательно, а работает параллельно благодаря механизму внимания. Эта особенность позволяет значительно ускорить обучение и эффективно моделировать длинные зависимости в тексте.

### 2. Механизм внимания  
Ключевой элемент трансформера — механизм внимания. Он позволяет модели «обращать внимание» на разные части входной последовательности при генерации каждого слова на выходе.

- **Scaled Dot-Product Attention:**  
  Для заданного запроса (Q), ключей (K) и значений (V) рассчитываются оценки внимания как:
  
  \[
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
  \]
  
  Здесь деление на \(\sqrt{d_k}\) (где \(d_k\) — размерность векторов ключей) помогает стабилизировать градиенты при больших значениях.

- **Многопоточное внимание (Multi-Head Attention):**  
  Вместо того чтобы вычислять одно большое матричное произведение, модель разбивает векторы на несколько «голов». Каждая голова обучается на своём подпространстве признаков, после чего результаты объединяются. Это позволяет модели захватывать различные типы зависимостей и аспекты информации.

### 3. Структура энкодера и декодера  
Transformer состоит из двух основных частей:

- **Энкодер:**  
  Состоит из нескольких одинаковых слоев. Каждый слой включает:
  - **Self-attention: - само-внимание** Позволяет каждому элементу входной последовательности взаимодействовать со всеми остальными.
  - **Feed-Forward Network - сеть прямой связи:** Нелинейный блок, применяемый к каждому элементу отдельно.
  - **Residual connections (остаточное соединение) и Layer Normalization (нормализация слоёв):** Для стабилизации обучения и предотвращения затухания градиентов.

- **Декодер:**  
  Также состоит из нескольких одинаковых слоев, но с двумя механизмами внимания:
  - **Self-attention - само-внимание:** С маскированием будущих позиций (чтобы предотвратить «подглядывание» на будущее).
  - **Encoder-Decoder attention:** Здесь декодер обращается к выходу энкодера, чтобы учитывать информацию исходной последовательности.
  
  После этих слоёв следует линейный слой и Softmax для генерации вероятностей по каждому слову целевого словаря.

### 4. Позиционное кодирование  
Поскольку Transformer не имеет рекуррентной структуры, он не знает порядка токенов. Для передачи информации о позиции используется позиционное кодирование. Чаще всего применяется синусоидальное кодирование, при котором каждой позиции соответствует вектор, вычисляемый с помощью синусов и косинусов разных частот. Это позволяет модели учитывать относительное расположение слов в предложении.

### 5. Обучение с Teacher Forcing  
На этапе обучения для декодера применяется техника «teacher forcing», когда на каждом шаге в качестве входа подаётся правильное (истинное) слово из целевой последовательности. Это помогает модели быстрее сходиться, поскольку она не должна полагаться на собственные предсказания на ранних этапах обучения.

---

## II. Объяснение работы кода

Наш код состоит из нескольких основных частей. Ниже приведено подробное объяснение каждой секции.

### 1. Подготовка данных и токенизация
- **Построение словаря:**  
  Функция `build_vocab` принимает список предложений и создаёт словарь, в котором каждому уникальному токену присваивается свой индекс. При этом добавляются специальные токены: `<pad>`, `<sos>`, `<eos>`, `<unk>`.
  
- **Токенизация:**  
  Функция `tokenize` преобразует предложение в последовательность индексов, добавляя токены начала (`<sos>`) и конца (`<eos>`).

### 2. Датасет и DataLoader
- **Класс `TranslationDataset`:**  
  Этот класс принимает списки исходных (на русском) и целевых (на английском) предложений, а также словари для каждого языка. Метод `__getitem__` возвращает тензоры с индексами токенов для пары предложений.
  
- **Функция `collate_fn`:**  
  Используется для формирования батчей. Она дополняет (паддинг) последовательности до одинаковой длины по размеру батча, что удобно для пакетной обработки.

### 3. Позиционное кодирование
- **Класс `PositionalEncoding`:**  
  Реализует синусоидальное позиционное кодирование. Оно добавляется к эмбеддингам, чтобы модель получала информацию о позиции каждого токена в последовательности.

### 4. Механизм внимания
- **Scaled Dot-Product Attention:**  
  Функция `scaled_dot_product_attention` вычисляет матрицу внимания для запросов, ключей и значений. Маска (если она передана) позволяет игнорировать определённые позиции (например, будущие токены в декодере).

- **Multi-Head Attention:**  
  Класс `MultiHeadAttention` разбивает входные данные на несколько голов, вычисляет внимание для каждой и затем объединяет результаты через линейный слой.

### 5. Feed-Forward блок
- **Класс `PositionwiseFeedForward`:**  
  Состоит из двух линейных слоёв с активацией ReLU между ними. Применяется к каждому элементу последовательности независимо.

### 6. Слои энкодера и декодера
- **`TransformerEncoderLayer`:**  
  Каждый слой энкодера включает модуль самовнимания (self-attention) с последующим residual соединением и нормализацией, затем блок feed-forward с аналогичными residual соединениями и нормализацией.

- **`TransformerDecoderLayer`:**  
  Каждый слой декодера содержит два модуля внимания:
  - **Self-attention с маскированием:** Позволяет декодеру учитывать только уже сгенерированные токены.
  - **Encoder-Decoder attention:** Использует выход энкодера для фокусировки на соответствующих частях входной последовательности.
  
  Далее следует блок feed-forward с residual соединениями и нормализацией.

### 7. Стеки энкодера и декодера
- **`TransformerEncoder` и `TransformerDecoder`:**  
  Представляют собой стеки из нескольких вышеописанных слоёв, что позволяет моделировать более сложные зависимости в данных.

### 8. Полная модель Transformer
- **Класс `TransformerModel`:**  
  Объединяет эмбеддинги для входного и выходного языков, позиционное кодирование, энкодер и декодер, а также линейный слой, преобразующий выход декодера в логиты для каждого слова целевого словаря.  
  В методе `forward` сначала создаются эмбеддинги, затем проходит энкодер, после чего декодер генерирует выход, который преобразуется линейным слоем.

### 9. Маскирование в декодере
- **Функция `generate_square_subsequent_mask`:**  
  Создаёт маску, запрещающую декодеру «подглядывать» будущие токены. Это важно при генерации последовательности, чтобы на каждом шаге модель опиралась только на уже сгенерированные слова.

### 10. Процедура перевода (Inference)
- **Функция `translate`:**  
  Принимает на вход модель, исходное предложение и словари. Сначала предложение токенизируется и пропускается через энкодер. Затем с помощью жадного декодирования (greedy decoding) последовательно генерируются токены перевода до появления токена конца последовательности (`<eos>`).

### 11. Главная функция обучения и тестового режима
- **Функция `main`:**  
  - **Подготовка датасета:** Здесь мы используем 100 пар предложений (полученных повторением 10 базовых примеров) для обучения модели перевода с русского на английский.
  - **Построение DataLoader:** Данные разбиваются на батчи для эффективного обучения.
  - **Параметры модели и обучение:** Задаются параметры модели (размерность эмбеддингов, число слоёв, количество голов и т.д.). На каждом шаге обучения применяется teacher forcing, при котором вход декодера — это сдвинутая целевая последовательность.
  - **Режим перевода:** После завершения обучения модель переводит введённое пользователем русское предложение на английский.

---

## Заключение

Архитектура Transformer, благодаря механизму внимания, позиционному кодированию и параллельной обработке, значительно ускоряет обучение и позволяет эффективно моделировать зависимости в последовательностях. Наш код реализует все ключевые компоненты Transformer: от предварительной обработки данных и формирования батчей до реализации слоёв внимания, энкодера и декодера, а также процедуры генерации перевода. Такой подход может быть масштабирован и до более сложных задач, если использовать расширенные и разнообразные датасеты.

