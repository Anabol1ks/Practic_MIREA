Ниже представлено подробное пошаговое объяснение кода, которое поможет понять его структуру и работу каждого блока.

---

### 1. Импорт библиотек и подготовка окружения

```python
import os
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import numpy as np
```

- **os** – модуль для работы с файловой системой (создание папок для логов).
- **torch и torch.nn, torch.optim** – основные модули PyTorch для создания нейронных сетей, определения слоёв, оптимизаторов и работы с тензорами.
- **torchvision и torchvision.transforms** – используются для работы с популярными датасетами (в данном случае CIFAR-10) и для предобработки изображений (аугментация, нормализация).
- **matplotlib.pyplot** – применяется для визуализации результатов (построение графиков потерь и точности, сохранение изображений с предсказаниями).
- **numpy** – библиотека для числовых операций, используется при преобразовании изображений.

---

### 2. Создание папки для логов

```python
logs_dir = "C:/Users/Grigo/Documents/Work/Practic_MIREA/semestr-6/Neyronki/cursach/logs"
os.makedirs(logs_dir, exist_ok=True)
```

- **os.makedirs** создаёт директорию для сохранения логов и графиков, если её нет. Это позволяет хранить все сгенерированные изображения (например, графики обучения, примеры предсказаний) в одном месте.

---

### 3. Определение устройства выполнения (CPU или GPU)

```python
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
```

- Код проверяет наличие CUDA-совместимого GPU и, если он доступен, выбирает GPU для ускорения вычислений. Если GPU недоступен, используется процессор (CPU).

---

### 4. Гиперпараметры обучения

```python
num_epochs = 20
batch_size = 128
learning_rate = 1e-3
```

- **num_epochs** – число эпох обучения (сколько раз модель пройдет по всему датасету).
- **batch_size** – размер мини-батча (количество примеров, обрабатываемых за одну итерацию).
- **learning_rate** – скорость обучения, регулирующая размер шага при обновлении параметров модели.

---

### 5. Подготовка датасета CIFAR-10 и аугментация

#### Преобразования для обучающей и тестовой выборки:

```python
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])
transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])
```

- **RandomCrop** – случайное вырезание изображения с отступом, что увеличивает разнообразие примеров.
- **RandomHorizontalFlip** – случайное горизонтальное отражение.
- **ToTensor** – преобразует изображение из PIL или numpy в тензор.
- **Normalize** – нормализация каналов изображения. Значения средних и стандартных отклонений указаны для CIFAR-10.

#### Загрузка датасета:

```python
train_dataset = torchvision.datasets.CIFAR10(
    root='C:/Users/Grigo/Documents/Work/Practic_MIREA/semestr-6/Neyronki/cursach',
    train=True, download=True, transform=transform_train)
test_dataset = torchvision.datasets.CIFAR10(
    root='C:/Users/Grigo/Documents/Work/Practic_MIREA/semestr-6/Neyronki/cursach',
    train=False, download=True, transform=transform_test)
```

- **CIFAR10** – стандартный датасет для задач классификации изображений, содержащий 10 классов.
- **root** – путь, по которому будут сохранены данные.
- **train=True/False** – выбирает обучающую или тестовую выборку.
- **download=True** – если датасет не найден, он будет загружен.

#### Создание DataLoader:

```python
train_loader = torch.utils.data.DataLoader(
    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
test_loader = torch.utils.data.DataLoader(
    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
```

- **DataLoader** – обеспечивает итерацию по датасету в мини-батчах.
- **shuffle=True** – перемешивает данные для обучающей выборки.
- **num_workers=2** – число потоков, используемых для загрузки данных.

---

### 6. Определение архитектуры моделей

#### 6.1. Сверточная нейросеть (CNN)

```python
class CNNClassifier(nn.Module):
    def __init__(self, num_classes=10):
        super(CNNClassifier, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),  # [B,32,32,32]
            nn.ReLU(),
            nn.MaxPool2d(2),                             # [B,32,16,16]
            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # [B,64,16,16]
            nn.ReLU(),
            nn.MaxPool2d(2),                             # [B,64,8,8]
            nn.Conv2d(64, 128, kernel_size=3, padding=1), # [B,128,8,8]
            nn.ReLU(),
            nn.MaxPool2d(2)                              # [B,128,4,4]
        )
        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(128 * 4 * 4, 256),
            nn.ReLU(),
            nn.Linear(256, num_classes)
        )
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x
```

- **Слои Conv2d, ReLU, MaxPool2d** – типичная комбинация для сверточного блока: свёртка извлекает признаки, ReLU добавляет нелинейность, max-pooling уменьшает пространственное разрешение.
- **Flatten и Linear** – после извлечения признаков свёрточной частью данные преобразуются в вектор, который передаётся в полносвязные слои для классификации.
- **num_classes=10** – выходной размер соответствует 10 классам CIFAR-10.

#### 6.2. Vision Transformer (ViT)

```python
class VisionTransformer(nn.Module):
    def __init__(self, image_size=32, patch_size=4, in_channels=3, num_classes=10,
                 embed_dim=128, num_layers=4, num_heads=4, dropout=0.1):
        super(VisionTransformer, self).__init__()
        assert image_size % patch_size == 0, "Размер изображения должен делиться на размер патча"
        self.num_patches = (image_size // patch_size) ** 2
        self.patch_dim = in_channels * patch_size * patch_size

        # Patch embedding
        self.patch_embed = nn.Linear(self.patch_dim, embed_dim)

        # Классификационный токен
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        # Позиционные эмбеддинги
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))
        self.dropout = nn.Dropout(dropout)

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # Классификационная голова
        self.mlp_head = nn.Sequential(
            nn.LayerNorm(embed_dim),
            nn.Linear(embed_dim, num_classes)
        )
    
    def forward(self, x):
        B, C, H, W = x.shape
        patch_size = int(np.sqrt(self.patch_dim / C))
        # Разбивка изображения на патчи:
        # Метод unfold делит изображение на неперекрывающиеся патчи
        x = x.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)
        x = x.contiguous().view(B, C, -1, patch_size, patch_size)  # [B, C, num_patches, patch_size, patch_size]
        x = x.permute(0, 2, 1, 3, 4).contiguous().view(B, self.num_patches, -1)  # [B, num_patches, patch_dim]

        # Применяем линейное преобразование к каждому патчу
        x = self.patch_embed(x)  # [B, num_patches, embed_dim]

        # Добавляем классификационный токен в начало последовательности
        cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, embed_dim]
        x = torch.cat((cls_tokens, x), dim=1)  # [B, num_patches+1, embed_dim]

        # Добавляем позиционные эмбеддинги и применяем dropout
        x = x + self.pos_embed
        x = self.dropout(x)

        # Трансформер ожидает вход в формате [seq_len, B, embed_dim], поэтому транспонируем
        x = x.transpose(0, 1)
        x = self.transformer_encoder(x)
        x = x.transpose(0, 1)  # возвращаем обратно формат [B, num_patches+1, embed_dim]

        # Используем эмбеддинг классификационного токена для классификации
        x = x[:, 0]
        x = self.mlp_head(x)
        return x
```

- **Patch embedding:** Изображение делится на небольшие патчи (здесь размер патча равен 4×4), каждый патч представляется в виде вектора (patch_dim = 3×4×4 = 48) и затем преобразуется линейным слоем в embedding размерности embed_dim.
- **cls_token:** Специальный токен, добавляемый в начало последовательности для получения итогового представления, используемого при классификации.
- **Позиционные эмбеддинги:** Добавляются к каждому эмбеддингу, чтобы сохранить информацию о порядке патчей.
- **Transformer Encoder:** Несколько слоев стандартного TransformerEncoder обрабатывают последовательность патчей.
- **MLP Head:** Финальный блок из нормализации и линейного слоя для получения логитов классов.

---

### 7. Функции обучения и оценки

#### 7.1. Функция обучения с сбором статистики

```python
def train_model(model, optimizer, criterion, train_loader, test_loader, num_epochs):
    model.train()
    losses = []
    acc_list = []
    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * images.size(0)
        epoch_loss = running_loss / len(train_loader.dataset)
        losses.append(epoch_loss)
        # Вычисление точности на тестовой выборке после каждой эпохи
        acc = evaluate_model(model, test_loader)
        acc_list.append(acc)
        print(f"Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f} Test Acc: {acc:.2f}%")
    return losses, acc_list
```

- **Цикл по эпохам и батчам:** На каждой эпохе модель обучается по всему датасету.
- **Обнуление градиентов, прямой проход, вычисление loss, обратное распространение ошибки и шаг оптимизатора – стандартная схема обучения в PyTorch.**
- **В конце каждой эпохи вычисляется точность на тестовой выборке и сохраняется значение loss и acc для построения графиков.**

#### 7.2. Функция тестирования модели

```python
def evaluate_model(model, test_loader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    acc = 100 * correct / total
    return acc
```

- **model.eval()** – перевод модели в режим оценки (выключаются dropout и batch normalization).
- **torch.no_grad()** – отключение вычисления градиентов для ускорения и экономии памяти.
- **Вычисляется точность классификации, сравнивая предсказанные и истинные метки.**

---

### 8. Получение и сохранение примеров предсказаний

#### 8.1. Функция получения примеров

```python
def get_prediction_examples(model, test_loader, num_examples=1):
    model.eval()
    correct_examples = []
    incorrect_examples = []
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            for i in range(images.size(0)):
                if predicted[i] == labels[i] and len(correct_examples) < num_examples:
                    correct_examples.append((images[i].cpu(), labels[i].cpu(), predicted[i].cpu()))
                elif predicted[i] != labels[i] and len(incorrect_examples) < num_examples:
                    incorrect_examples.append((images[i].cpu(), labels[i].cpu(), predicted[i].cpu()))
                if len(correct_examples) >= num_examples and len(incorrect_examples) >= num_examples:
                    break
            if len(correct_examples) >= num_examples and len(incorrect_examples) >= num_examples:
                break
    return correct_examples, incorrect_examples
```

- Функция перебирает батчи из тестовой выборки и собирает примеры, где предсказание модели совпадает с истинной меткой (correct) и не совпадает (incorrect).
- **num_examples** позволяет задать, сколько примеров каждого типа собрать.

#### 8.2. Функция денормализации изображения

```python
def unnormalize(img, mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010)):
    for t, m, s in zip(img, mean, std):
        t.mul_(s).add_(m)
    return img
```

- Производится обратное преобразование нормализации, чтобы восстановить исходные значения пикселей для корректного отображения изображения.

#### 8.3. Функция сохранения примера предсказания

```python
def save_prediction_example(example, filename, model_name):
    # example: (image, true_label, predicted_label)
    img, true_label, pred_label = example
    img = unnormalize(img)
    npimg = img.numpy().transpose(1, 2, 0)
    plt.figure()
    plt.imshow(np.clip(npimg, 0, 1))
    plt.title(f"{model_name} | True: {true_label.item()} Pred: {pred_label.item()}")
    plt.axis("off")
    plt.savefig(os.path.join(logs_dir, filename))
    plt.close()
```

- Преобразует тензор в формат, подходящий для отображения (numpy-массив с нужной размерностью).
- Добавляет подпись с указанием модели, истинной метки и предсказанной метки.
- Сохраняет изображение в указанную директорию.

---

### 9. Основной блок выполнения

```python
if __name__ == '__main__':
    # Инициализация моделей, оптимизаторов и функции потерь
    cnn_model = CNNClassifier(num_classes=10).to(device)
    vit_model = VisionTransformer(image_size=32, patch_size=4, in_channels=3, num_classes=10,
                                  embed_dim=128, num_layers=4, num_heads=4, dropout=0.1).to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer_cnn = optim.Adam(cnn_model.parameters(), lr=learning_rate)
    optimizer_vit = optim.Adam(vit_model.parameters(), lr=learning_rate)

    # Обучение и оценка сверточной нейросети
    print("Обучение сверточной нейросети...")
    cnn_losses, cnn_acc_list = train_model(cnn_model, optimizer_cnn, criterion, train_loader, test_loader, num_epochs)
    final_cnn_acc = evaluate_model(cnn_model, test_loader)
    print(f"Точность CNN: {final_cnn_acc:.2f}%")

    # Обучение и оценка модели Vision Transformer
    print("\nОбучение Vision Transformer...")
    vit_losses, vit_acc_list = train_model(vit_model, optimizer_vit, criterion, train_loader, test_loader, num_epochs)
    final_vit_acc = evaluate_model(vit_model, test_loader)
    print(f"Точность ViT: {final_vit_acc:.2f}%")

    # Сохранение графиков обучения
    # Рисунок 4.1 – График потерь сверточной нейросети
    plt.figure()
    plt.plot(range(1, num_epochs+1), cnn_losses, marker='o')
    plt.xlabel("Эпоха")
    plt.ylabel("Loss")
    plt.title("Рисунок 4.1 – Потери CNN")
    plt.grid(True)
    plt.savefig(os.path.join(logs_dir, "CNN_loss.png"))
    plt.close()

    # Рисунок 4.2 – График точности сверточной нейросети
    plt.figure()
    plt.plot(range(1, num_epochs+1), cnn_acc_list, marker='o', color='green')
    plt.xlabel("Эпоха")
    plt.ylabel("Accuracy (%)")
    plt.title("Рисунок 4.2 – Точность CNN")
    plt.grid(True)
    plt.savefig(os.path.join(logs_dir, "CNN_accuracy.png"))
    plt.close()

    # Рисунок 4.3 – График потерь трансформерной модели
    plt.figure()
    plt.plot(range(1, num_epochs+1), vit_losses, marker='o', color='red')
    plt.xlabel("Эпоха")
    plt.ylabel("Loss")
    plt.title("Рисунок 4.3 – Потери ViT")
    plt.grid(True)
    plt.savefig(os.path.join(logs_dir, "ViT_loss.png"))
    plt.close()

    # Рисунок 4.4 – График точности трансформерной модели
    plt.figure()
    plt.plot(range(1, num_epochs+1), vit_acc_list, marker='o', color='purple')
    plt.xlabel("Эпоха")
    plt.ylabel("Accuracy (%)")
    plt.title("Рисунок 4.4 – Точность ViT")
    plt.grid(True)
    plt.savefig(os.path.join(logs_dir, "ViT_accuracy.png"))
    plt.close()

    # Получаем примеры предсказаний для сверточной сети (CNN)
    cnn_correct, cnn_incorrect = get_prediction_examples(cnn_model, test_loader, num_examples=1)
    if cnn_correct:
        save_prediction_example(cnn_correct[0], "CNN_correct.png", "CNN")
    if cnn_incorrect:
        save_prediction_example(cnn_incorrect[0], "CNN_incorrect.png", "CNN")
    
    # Получаем примеры предсказаний для модели Vision Transformer (ViT)
    vit_correct, vit_incorrect = get_prediction_examples(vit_model, test_loader, num_examples=1)
    if vit_correct:
        save_prediction_example(vit_correct[0], "ViT_correct.png", "ViT")
    if vit_incorrect:
        save_prediction_example(vit_incorrect[0], "ViT_incorrect.png", "ViT")
```

- Основной блок кода обёрнут в конструкцию `if __name__ == '__main__':` – это необходимо для корректной работы с многопроцессорной загрузкой данных (особенно на Windows).
- Модели CNN и ViT инициализируются и переносятся на выбранное устройство (GPU или CPU).
- Задаётся функция потерь (CrossEntropyLoss) и оптимизаторы.
- После обучения для каждой модели:
  - Строятся графики потерь и точности (рисунки 4.1–4.4), сохраняемые в папку логов.
  - Выбираются примеры корректных и ошибочных предсказаний, и сохраняются изображения (рисунки 4.5–4.8) с подписью, указывающей модель, истинную и предсказанную метку.

---

### Заключение

Код реализует сравнительный эксперимент для двух архитектур: традиционной сверточной нейросети и Vision Transformer. Он охватывает полный цикл подготовки данных, построения моделей, обучения, оценки и визуализации результатов. Предусмотрена регистрация ключевых метрик (функция потерь и точность) для каждой эпохи, построение графиков, а также сохранение примеров предсказаний для анализа работы модели.

Если понадобятся дополнительные пояснения или доработки, дайте знать!